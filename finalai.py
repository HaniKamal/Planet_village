# -*- coding: utf-8 -*-
"""FinalAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X_8FkMbtMvesgs2EWKvjEhFB9inTAKEr
"""

# import necessary packages

import pandas
import opendatasets as od
from sklearn.preprocessing import LabelEncoder
import mahotas
import cv2
import os
import h5py
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_validate
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
import numpy as np
import pathlib
from glob import glob
from tqdm import tqdm
import matplotlib.pyplot as plt
from PIL import Image
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import *

# {"Kaggle username":"hanikamal","Kaggle Key":"bb3040d21aad356e7ab7c1bc5548b26c"}

od.download(
    "https://www.kaggle.com/datasets/hanikamal/uni-plant-village")

def getListOfFiles(dirName):
    # create a list of file and sub directories
    # names in the given directory
    listOfFile = os.listdir(dirName)
    allFiles = list()
    # Iterate over all the entries
    for entry in listOfFile:
        # Create full path
        fullPath = os.path.join(dirName, entry)
        # If entry is a directory then get the list of files in this directory
        if os.path.isdir(fullPath):
            allFiles = allFiles + getListOfFiles(fullPath)
        else:
            allFiles.append(fullPath)

    return allFiles

# loading training dataset
training_dir = './uni-plant-village/planet_village/train'
validation_dir = './uni-plant-village/planet_village/valid'

imagePaths = getListOfFiles(training_dir)

'''*******************************************************
________________Image Proccesing Functions_____________
*******************************************************'''

# feature-descriptor-1: Hu Moments
def fd_hu_moments(image):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    feature = cv2.HuMoments(cv2.moments(image)).flatten()
    return feature

# feature-descriptor-2: Haralick Texture
def fd_haralick(image):
    # convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # compute the haralick texture feature vector
    haralick = mahotas.features.haralick(gray).mean(axis=0)
    # return the result
    return haralick

# feature-descriptor-3: Color Histogram
bins = 8
def fd_histogram(image, mask=None):
    # convert the image to HSV color-space
    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    # compute the color histogram
    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])
    # normalize the histogram
    cv2.normalize(hist, hist)
    # return the histogram
    return hist.flatten()

# import the necessary packages
import cv2
import imutils
from google.colab.patches import cv2_imshow
  
# load the input image and convert it to grayscale
image = cv2.imread(
    "/content/uni-plant-village/planet_village/test/PotatoEarlyBlight2.JPG")
image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
  
 
# find the contours of the three planes in the image
cnts = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL,
    cv2.CHAIN_APPROX_SIMPLE)
cnts = imutils.grab_contours(cnts)
  
# loop over each contour
for (i, c) in enumerate(cnts):
    # extract the ROI from the image and compute the Hu Moments feature
    # vector for the ROI
    (x, y, w, h) = cv2.boundingRect(c)
    roi = image[y:y + h, x:x + w]
    moments = cv2.HuMoments(cv2.moments(roi)).flatten()
  
    # show the moments and ROI
    print("MOMENTS are #{}: {}".format(i + 1, moments))
    cv2_imshow( roi)
    cv2.waitKey(0)

# importing required libraries
import numpy as np
import mahotas
from pylab import imshow, show
  
# loading image
img = mahotas.imread(
    '/content/uni-plant-village/planet_village/test/PotatoEarlyBlight2.JPG')
    
# filtering the image
img = img[:, :, 0]
     
# setting gaussian filter
gaussian = mahotas.gaussian_filter(img, 15)
  
# setting threshold value
gaussian = (gaussian > gaussian.mean())
  
# making is labelled image
labeled, n = mahotas.label(gaussian)
 
# showing image
print("Labelled Image")
imshow(labeled)
show()
 
# getting haralick features
h_feature = mahotas.features.haralick(labeled)
 
# showing the feature
print("Haralick Features")
imshow(h_feature)
show()

import cv2
import numpy as np
from matplotlib import pyplot as plt
  
img = cv2.imread('/content/uni-plant-village/planet_village/test/PotatoEarlyBlight2.JPG')

# Calculate histogram without mask
hist1 = cv2.calcHist([img],[0],None,[256],[0,256])
hist2 = cv2.calcHist([img],[1],None,[256],[0,256])
hist3 = cv2.calcHist([img],[2],None,[256],[0,256])

plt.subplot(221), plt.imshow(img)
plt.subplot(222), plt.plot(hist1), plt.plot(hist2),plt.plot(hist3)
plt.xlim([0,256])

plt.show()

# check size of a random sample pic
import glob, random
file_path_type = ["./uni-plant-village/planet_village/train/*/*.JPG"]
images = glob.glob(random.choice(file_path_type))
random_image = random.choice(images)
im = Image.open(random_image)
width, height = im.size
print(f'The width, height = {width} x {height}')
plt.imshow(im)

'''*******************************************************
________________Data / Images preperation _____________
*******************************************************'''
# empty lists to hold feature vectors and labels
data = []
lables = []
# loop over the images in each sub-folder
for image in imagePaths:
    lable = os.path.split(os.path.split(image)[0])[1]
    lables.append(lable)
    # read the image and resize it to a fixed-size
    img = cv2.imread(image)
    img = cv2.resize(img, (32, 32), interpolation=cv2.INTER_AREA)
    # Global Feature extraction
    fv_hu_moments = fd_hu_moments(img )
    fv_haralick   = fd_haralick(img )
    fv_histogram  = fd_histogram(img )
    # Concatenate global features
    img_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])
    # update the list of feature vectors to dataset
    data.append(img_feature)

# scale features in the range (0-1)
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data)

print(data)

# encode the labels as integer
data = np.array(data)

lables = np.array(lables)
class_names = lables 

le = LabelEncoder()
lables = le.fit_transform(lables)

myset = set(lables)
print(myset)

dataset_size = data.shape[0]
data = data.reshape(dataset_size, -1)
print(data.shape)
print(lables.shape)
print(dataset_size)

# Number of images for each disease
import pandas as pd

print("Total disease classes are: {}".format(len(class_names)))
nums = {}
for disease in class_names:
    nums[disease] = len(os.listdir(training_dir+ '/' + disease))
    
# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column

img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=["no. of images"])
img_per_class

# plotting number of images available for each disease
index = [n for n in range(19)]
plt.figure(figsize=(20, 5))
plt.bar(index, [n for n in nums.values()], width=0.3)
plt.xlabel('Plants/Diseases', fontsize=11)
plt.ylabel('No of images available', fontsize=10)
plt.xticks(index, class_names, fontsize=12, rotation=90)
plt.title('Images per each class of plant disease')

(X_train, X_test, y_train, y_test) = train_test_split(
    data, lables, test_size=0.2, random_state=42, shuffle= True)

print("Splitted train and test data...")
print("Train data  : {}".format(X_train.shape))
print("Test data   : {}".format(y_train.shape))
print("Train labels: {}".format(X_test.shape))
print("Test labels : {}".format(y_test.shape))

'''*******************************************************
________________RandomForest Model_____________
*******************************************************'''

from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier()

# Try different numbers of n_estimators
estimators = np.arange(10, 150, 10)
accuracy = []

for n in estimators:
    model.set_params(n_estimators=n)
    model.fit(X_train, y_train)
    y_pred=model.predict(X_test)
    accuracy.append(metrics.accuracy_score(y_test, y_pred))
plt.title("Effect of n_estimators")
plt.xlabel("n_estimator")
plt.ylabel("accuracy")
plt.plot(estimators, accuracy);

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
y_pred = model.predict(X_test)
accuracy_score(y_pred,y_test)
print(classification_report(y_pred,y_test))
print('Percentage correct: ', round((100*np.sum(y_pred == y_test)/len(y_test)), 3))

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X=X_test, y=y_test, cv=10, n_jobs=1)
 
print('Cross Validation accuracy scores: %s' % scores)
 
print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.set(font_scale=0.75) # for label size
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cmn, annot=True, fmt='.2f')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show(block=False)

cv = cross_validate(model, X_test, y_test, cv=10)
print((np.round(cv['test_score'],3)*100))
print((np.round(cv['test_score'].mean(),3)*100))

# Function for Ploting the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """ 
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))
  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

'''*******************************************************
________________Data Augmentation for CNN_____________
*******************************************************'''
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)

train_ds = train_datagen.flow_from_directory(
    training_dir,
    class_mode='categorical',
    target_size=(124,124),
    batch_size=32,
    shuffle = True

)

valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

valid_ds = valid_datagen.flow_from_directory(
    validation_dir,
    class_mode='categorical',
    target_size=(124,124),
    batch_size=32,
    shuffle = True 
)

'''*******************************************************
________________Simple CNN model_____________
*******************************************************'''

# Set random seed and number of classes
tf.random.set_seed(42)
num_classes = 19
# create the simple CNN Model
model_cnn= tf.keras.Sequential([
  tf.keras.layers.Conv2D(10, 3, activation='relu',input_shape=(124, 124, 3)),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(10, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(100, activation="relu"),
  tf.keras.layers.Dense(num_classes, activation="softmax")
])
#Compile and Fit the modle
model_cnn.compile(
  optimizer='adam',
  loss='categorical_crossentropy',
  metrics=['accuracy'])
cnn_history = model_cnn.fit(
  train_ds,
  steps_per_epoch=len(train_ds),
  validation_data=valid_ds,
  validation_steps=len(valid_ds),
  epochs=12,
)


model_cnn.summary()

# Check out the loss curves of model
plot_loss_curves(cnn_history)

# You can also evaluate or predict on a dataset.
print("Evaluate")
result = model_cnn.evaluate(valid_ds)
dict(zip(model_cnn.metrics_names, result))

'''*******************************************************
________________VGG model_____________
*******************************************************'''
# Set random seed and number of classes
tf.random.set_seed(42)
num_classes = 19

from keras.callbacks import EarlyStopping
# Create the TinyVGG Model
model_TinyVGG= tf.keras.Sequential([
  tf.keras.layers.Conv2D(8,(3,3), activation='relu',input_shape=(124, 124, 3),padding='same'),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Conv2D(16,(3,3), activation='relu',padding='same'),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Conv2D(32,(3,3), activation='relu',padding='same'),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation="relu"),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(num_classes, activation="softmax")
])


#Compile and Fit the modle
model_TinyVGG.compile(
  optimizer='adam',
  loss='categorical_crossentropy',
  metrics=['accuracy'])
TinyVGG_history = model_TinyVGG.fit(
  train_ds,
  steps_per_epoch=len(train_ds),
  validation_data=valid_ds,
  validation_steps=len(valid_ds),
  epochs=20,
)

model_TinyVGG.summary()

# Check out the loss curves of model
plot_loss_curves(TinyVGG_history)

# You can also evaluate or predict on a dataset.
print("Evaluate")
result_VGG = model_TinyVGG.evaluate(valid_ds)
dict(zip(model_TinyVGG.metrics_names, result_VGG))